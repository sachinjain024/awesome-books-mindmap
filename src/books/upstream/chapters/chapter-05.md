---
number: 5
title: How Will You Know You've Succeeded?
meta: Measuring what doesn't happen
part: Building Upstream Systems
layout: chapter
book: upstream
permalink: upstream/chapters/chapter-05.html
---

> "Upstream thinkers must find ways to measure what didn't happen — the accident that didn't occur, the disease that didn't develop, the crime that wasn't committed."
> — Dan Heath

## The Measurement Challenge of Prevention

Measuring the success of upstream work is genuinely hard. The fundamental challenge: when prevention works, nothing happens. The outcome you're trying to prevent doesn't materialize. There is no event to count, no incident to log, no crisis to document.

Compare this to downstream work. When a firefighter extinguishes a fire, there is an event — a fire — and a measurable outcome — it was extinguished. The cause and effect are clear, immediate, and visible. Upstream work has none of these properties. The fire didn't start. Where is the data point for that?

This measurement problem is not just philosophical — it has real consequences. Upstream programs that prevent problems are harder to evaluate, harder to fund, and harder to defend to stakeholders than downstream programs that respond to crises. The cost of an ineffective downstream intervention is visible: the problem persists. The cost of cutting a successful upstream program is invisible: the problems start recurring, and it may take years before anyone connects the dots.

### Leading vs. Lagging Indicators {.principle-box}

The solution to measuring upstream success is to shift from lagging indicators to leading indicators.

**Lagging indicators** measure what already happened. Dropout rates, crime rates, disease incidence, accident rates — these record the outcome of processes that already played out. By the time you see the lagging indicator move, it's too late to prevent the specific events that moved it.

**Leading indicators** measure what predicts the outcome. If you know that students who miss more than five days in October are at high risk of dropping out by June, you can use October attendance as a leading indicator — and intervene before the dropout happens. The leading indicator lets you act while there's still time.

Effective upstream programs identify and track leading indicators that are:
- Measurable (you can reliably and consistently collect the data)
- Predictive (they actually correlate with the outcomes you care about)
- Actionable (you can actually influence them with available interventions)

## Iceland's Prevention Revolution

In the 1990s, Iceland had one of the highest rates of teen drug and alcohol use in Europe. The country tried the standard approaches — education programs, awareness campaigns, law enforcement — with limited success.

### The IYDC Framework {.insight-box}

Then researchers at the Iceland Youth Development Centre (IYDC) introduced a radical idea: instead of measuring drug use (a lagging indicator) and trying to reduce it, they would measure the underlying factors that protected against drug use (leading indicators) and try to increase them.

Their research identified several powerful protective factors:
- **Time with parents** — hours per week spent in genuine interaction with parents
- **Participation in organized activities** — sports clubs, arts programs, community organizations
- **Sleep** — hours per night on school nights
- **Sense of belonging** — feeling connected to school, community, and peers

Iceland then launched a national program to improve these leading indicators — investing in after-school sports and arts programs, campaigning for earlier school start times, and creating community spaces for teen activities. Drug and alcohol use among 15-16 year olds fell dramatically over the following decade, without the government ever targeting drug use directly.

The lesson is profound: by finding and improving the right leading indicators, Iceland prevented the outcome without ever confronting it directly.

## The Ghost of What Didn't Happen

One of the most poignant passages in *Upstream* is Heath's discussion of the people who weren't harmed — the victims who don't exist because an upstream program worked. These people never know they were at risk. They never know an intervention protected them. They show up nowhere in the statistics.

### Making the Invisible Visible {.insight-box}

How do you make a compelling case for upstream investment when the success is invisible? Heath suggests several approaches:

- **Counterfactual modeling:** estimate what would have happened without the program, based on trends in comparable populations
- **Pilot-and-compare:** run the program in some communities but not others, and measure the difference over time
- **Historical comparison:** compare outcomes before and after the program's introduction
- **Leading indicator tracking:** show that the leading indicators you targeted are improving, and demonstrate their historical correlation with the lagging outcomes you care about

None of these approaches is perfect. But they make the invisible visible enough to justify the investment.

### Reflection {.reflection}

For a program or initiative you're currently running or considering, what is the leading indicator that would tell you six months from now whether you're on track? How would you measure it, and how confident are you in its predictive power?

## Key Takeaways

- The fundamental measurement challenge of upstream work: when prevention succeeds, nothing happens — there is no event to count
- Lagging indicators measure what already happened (dropout rates, disease incidence, crime rates) — they tell you about the past, too late to intervene
- Leading indicators measure what predicts the outcome — they give you time to act while the outcome is still preventable
- Iceland's teen drug prevention program succeeded by measuring and improving protective factors (time with parents, organized activities, sleep) rather than targeting drug use directly
- Effective leading indicators must be measurable, predictive, and actionable — all three conditions are necessary
- Making the case for upstream programs requires counterfactual modeling, pilot comparisons, or historical analysis to make the "ghost of what didn't happen" visible to funders and policymakers
