<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 5: How Will You Know You&#39;ve Succeeded? - </title>
    <link rel="stylesheet" href="../../shared/styles.css">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    
    <div class="container container-narrow">
        <nav class="breadcrumb">
            <a href="../../index.html" class="breadcrumb-link">üè†</a>
            <span class="breadcrumb-separator">‚Ä∫</span>
            <a href="../index.html" class="breadcrumb-link">Measuring what doesn&#39;t happen</a>
            <span class="breadcrumb-separator">‚Ä∫</span>
            <span class="breadcrumb-current">Chapter 5</span>
        </nav>

        <h1 class="chapter-title">How Will You Know You&#39;ve Succeeded?</h1>
        
        <div class="chapter-meta">Measuring what doesn&#39;t happen</div>
        

        <div class="chapter-content">
            <blockquote>
<p>‚ÄúUpstream thinkers must find ways to measure what didn‚Äôt happen ‚Äî the accident that didn‚Äôt occur, the disease that didn‚Äôt develop, the crime that wasn‚Äôt committed.‚Äù
‚Äî Dan Heath</p>
</blockquote>
<h2>The Measurement Challenge of Prevention</h2>
<p>Measuring the success of upstream work is genuinely hard. The fundamental challenge: when prevention works, nothing happens. The outcome you‚Äôre trying to prevent doesn‚Äôt materialize. There is no event to count, no incident to log, no crisis to document.</p>
<p>Compare this to downstream work. When a firefighter extinguishes a fire, there is an event ‚Äî a fire ‚Äî and a measurable outcome ‚Äî it was extinguished. The cause and effect are clear, immediate, and visible. Upstream work has none of these properties. The fire didn‚Äôt start. Where is the data point for that?</p>
<p>This measurement problem is not just philosophical ‚Äî it has real consequences. Upstream programs that prevent problems are harder to evaluate, harder to fund, and harder to defend to stakeholders than downstream programs that respond to crises. The cost of an ineffective downstream intervention is visible: the problem persists. The cost of cutting a successful upstream program is invisible: the problems start recurring, and it may take years before anyone connects the dots.</p>
<h3 class="principle-box">Leading vs. Lagging Indicators</h3>
<p>The solution to measuring upstream success is to shift from lagging indicators to leading indicators.</p>
<p><strong>Lagging indicators</strong> measure what already happened. Dropout rates, crime rates, disease incidence, accident rates ‚Äî these record the outcome of processes that already played out. By the time you see the lagging indicator move, it‚Äôs too late to prevent the specific events that moved it.</p>
<p><strong>Leading indicators</strong> measure what predicts the outcome. If you know that students who miss more than five days in October are at high risk of dropping out by June, you can use October attendance as a leading indicator ‚Äî and intervene before the dropout happens. The leading indicator lets you act while there‚Äôs still time.</p>
<p>Effective upstream programs identify and track leading indicators that are:</p>
<ul>
<li>Measurable (you can reliably and consistently collect the data)</li>
<li>Predictive (they actually correlate with the outcomes you care about)</li>
<li>Actionable (you can actually influence them with available interventions)</li>
</ul>
<h2>Iceland‚Äôs Prevention Revolution</h2>
<p>In the 1990s, Iceland had one of the highest rates of teen drug and alcohol use in Europe. The country tried the standard approaches ‚Äî education programs, awareness campaigns, law enforcement ‚Äî with limited success.</p>
<h3 class="insight-box">The IYDC Framework</h3>
<p>Then researchers at the Iceland Youth Development Centre (IYDC) introduced a radical idea: instead of measuring drug use (a lagging indicator) and trying to reduce it, they would measure the underlying factors that protected against drug use (leading indicators) and try to increase them.</p>
<p>Their research identified several powerful protective factors:</p>
<ul>
<li><strong>Time with parents</strong> ‚Äî hours per week spent in genuine interaction with parents</li>
<li><strong>Participation in organized activities</strong> ‚Äî sports clubs, arts programs, community organizations</li>
<li><strong>Sleep</strong> ‚Äî hours per night on school nights</li>
<li><strong>Sense of belonging</strong> ‚Äî feeling connected to school, community, and peers</li>
</ul>
<p>Iceland then launched a national program to improve these leading indicators ‚Äî investing in after-school sports and arts programs, campaigning for earlier school start times, and creating community spaces for teen activities. Drug and alcohol use among 15-16 year olds fell dramatically over the following decade, without the government ever targeting drug use directly.</p>
<p>The lesson is profound: by finding and improving the right leading indicators, Iceland prevented the outcome without ever confronting it directly.</p>
<h2>The Ghost of What Didn‚Äôt Happen</h2>
<p>One of the most poignant passages in <em>Upstream</em> is Heath‚Äôs discussion of the people who weren‚Äôt harmed ‚Äî the victims who don‚Äôt exist because an upstream program worked. These people never know they were at risk. They never know an intervention protected them. They show up nowhere in the statistics.</p>
<h3 class="insight-box">Making the Invisible Visible</h3>
<p>How do you make a compelling case for upstream investment when the success is invisible? Heath suggests several approaches:</p>
<ul>
<li><strong>Counterfactual modeling:</strong> estimate what would have happened without the program, based on trends in comparable populations</li>
<li><strong>Pilot-and-compare:</strong> run the program in some communities but not others, and measure the difference over time</li>
<li><strong>Historical comparison:</strong> compare outcomes before and after the program‚Äôs introduction</li>
<li><strong>Leading indicator tracking:</strong> show that the leading indicators you targeted are improving, and demonstrate their historical correlation with the lagging outcomes you care about</li>
</ul>
<p>None of these approaches is perfect. But they make the invisible visible enough to justify the investment.</p>
<h3 class="reflection">Reflection</h3>
<p>For a program or initiative you‚Äôre currently running or considering, what is the leading indicator that would tell you six months from now whether you‚Äôre on track? How would you measure it, and how confident are you in its predictive power?</p>
<h2>Key Takeaways</h2>
<ul>
<li>The fundamental measurement challenge of upstream work: when prevention succeeds, nothing happens ‚Äî there is no event to count</li>
<li>Lagging indicators measure what already happened (dropout rates, disease incidence, crime rates) ‚Äî they tell you about the past, too late to intervene</li>
<li>Leading indicators measure what predicts the outcome ‚Äî they give you time to act while the outcome is still preventable</li>
<li>Iceland‚Äôs teen drug prevention program succeeded by measuring and improving protective factors (time with parents, organized activities, sleep) rather than targeting drug use directly</li>
<li>Effective leading indicators must be measurable, predictive, and actionable ‚Äî all three conditions are necessary</li>
<li>Making the case for upstream programs requires counterfactual modeling, pilot comparisons, or historical analysis to make the ‚Äúghost of what didn‚Äôt happen‚Äù visible to funders and policymakers</li>
</ul>

        </div>

        <div class="chapter-nav">
            
            <a href="chapter-04.html">‚Üê Previous: Chapter 4</a>
            

            
            <a href="chapter-06.html">Next: Chapter 6 ‚Üí</a>
            
        </div>
    </div>

    <script src="../../shared/highlight.js" defer></script>
</body>
</html>
